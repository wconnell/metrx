{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(123)\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook settings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCGA-LUSC    301\n",
       "TCGA-LUAD    287\n",
       "CPTAC-3      209\n",
       "Name: Project ID, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = pd.read_csv('../data/TCGA/rna-seq_adeno/meta/gdc_sample_sheet.2020-01-27.tsv', sep=\"\\t\")\n",
    "# get file type\n",
    "samples['data'] = [val[1] for i,val in samples['File Name'].str.split(\".\").items()]\n",
    "samples['Project ID'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples with RNAseq adjacent normal tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Primary Tumor                               558\n",
       "Solid Tissue Normal                         206\n",
       "Primary Tumor, Primary Tumor                 29\n",
       "Solid Tissue Normal, Solid Tissue Normal      4\n",
       "Name: Sample Type, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['Sample Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.loc[samples['Sample Type']=='Primary Tumor, Primary Tumor', 'Sample Type'] = 'Primary Tumor'\n",
    "samples.loc[samples['Sample Type']=='Solid Tissue Normal, Solid Tissue Normal', 'Sample Type'] = 'Solid Tissue Normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Primary Tumor          587\n",
       "Solid Tissue Normal    210\n",
       "Name: Sample Type, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['Sample Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all cases with adjacent normal tissue\n",
    "cases = samples[samples['Sample Type']=='Solid Tissue Normal']['Case ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disparity in cases\n",
    "samples[(samples['Case ID'].isin(cases)) & (samples['Sample Type']=='Primary Tumor') \n",
    "        & (samples['data']=='FPKM')]['Case ID'].nunique()\n",
    "samples[(samples['Case ID'].isin(cases)) & (samples['Sample Type']=='Solid Tissue Normal') \n",
    "        & (samples['data']=='FPKM')]['Case ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# divide, join, subset\n",
    "case_tumor = samples[(samples['Case ID'].isin(cases)) & (samples['Sample Type']=='Primary Tumor') & \n",
    "                     (samples['data']=='FPKM')]\n",
    "case_norm = samples[(samples['Case ID'].isin(cases)) & (samples['Sample Type']=='Solid Tissue Normal') & \n",
    "                    (samples['data']=='FPKM')]\n",
    "cases = pd.merge(case_tumor['Case ID'], case_norm['Case ID'])['Case ID']\n",
    "cases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_tumor = case_tumor[case_tumor['Case ID'].isin(cases)]\n",
    "case_norm = case_norm[case_norm['Case ID'].isin(cases)]\n",
    "cases = pd.concat([case_tumor, case_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(176, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(363, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_tumor.shape\n",
    "case_norm.shape\n",
    "cases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Primary Tumor          140\n",
       "Solid Tissue Normal    132\n",
       "Name: Sample Type, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Primary Tumor          47\n",
       "Solid Tissue Normal    44\n",
       "Name: Sample Type, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "target = 'Sample Type'\n",
    "cases[target] = cases[target].astype('category')\n",
    "\n",
    "train, test = train_test_split(cases)\n",
    "train[target].value_counts()\n",
    "test[target].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f806209d850>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Primary Tumor': 0, 'Solid Tissue Normal': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "torch.manual_seed(123)\n",
    "\n",
    "from trainer import fit\n",
    "import visualization as vis\n",
    "import numpy as np\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"Cuda is available: {}\".format(cuda))\n",
    "\n",
    "classes = {key:val for val,key in enumerate(train[target].cat.categories.values)}\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcga_datasets import TCGA, SiameseTCGA\n",
    "root_dir = \"../data/TCGA/rna-seq_adeno/\"\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = TCGA(root_dir, samples=train, train=True, target=target, norm=False)\n",
    "test_dataset = TCGA(root_dir, samples=test, train=False, target=target, norm=False)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_dataset.data = pd.DataFrame(scaler.fit_transform(train_dataset.data),\n",
    "                                  index=train_dataset.data.index,\n",
    "                                  columns=train_dataset.data.columns)\n",
    "test_dataset.data = pd.DataFrame(scaler.transform(test_dataset.data),\n",
    "                                 index=test_dataset.data.index,\n",
    "                                 columns=test_dataset.data.columns)\n",
    "\n",
    "kwargs = {'num_workers': 10, 'pin_memory': True} if cuda else {'num_workers': 10}\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNet(\n",
       "  (embedding_net): EmbeddingNet(\n",
       "    (fc): Sequential(\n",
       "      (linear1): Linear(in_features=60483, out_features=2000, bias=True)\n",
       "      (relu1): PReLU(num_parameters=1)\n",
       "      (linear2): Linear(in_features=2000, out_features=500, bias=True)\n",
       "      (relu2): PReLU(num_parameters=1)\n",
       "      (linear3): Linear(in_features=500, out_features=250, bias=True)\n",
       "      (relu3): PReLU(num_parameters=1)\n",
       "      (linear4): Linear(in_features=250, out_features=100, bias=True)\n",
       "      (relu4): PReLU(num_parameters=1)\n",
       "      (linear5): Linear(in_features=100, out_features=50, bias=True)\n",
       "      (relu5): PReLU(num_parameters=1)\n",
       "      (linear6): Linear(in_features=50, out_features=10, bias=True)\n",
       "      (relu6): PReLU(num_parameters=1)\n",
       "      (linear7): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 set up dataloader\n",
    "root_dir = \"../data/TCGA\"\n",
    "siamese_train_dataset = SiameseTCGA(train_dataset) # Returns pairs of images and target same/different\n",
    "siamese_test_dataset = SiameseTCGA(test_dataset)\n",
    "batch_size = 8\n",
    "kwargs = {'num_workers': 10, 'pin_memory': True} if cuda else {}\n",
    "siamese_train_loader = torch.utils.data.DataLoader(siamese_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "siamese_test_loader = torch.utils.data.DataLoader(siamese_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "from tcga_networks import EmbeddingNet, SiameseNet\n",
    "from losses import ContrastiveLoss\n",
    "from metrics import AccumulatedAccuracyMetric\n",
    "\n",
    "# Step 2\n",
    "embedding_net = EmbeddingNet()\n",
    "# Step 3\n",
    "model = SiameseNet(embedding_net)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "# Step 4\n",
    "margin = 1.\n",
    "loss_fn = ContrastiveLoss(margin)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 50\n",
    "# print training metrics every log_interval * batch_size\n",
    "log_interval = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/272 (0%)]\tLoss: 0.186468\n",
      "Train: [240/272 (88%)]\tLoss: 0.573355\n",
      "Epoch: 1/50. Train set: Average loss: 0.5241\n",
      "Epoch: 1/50. Validation set: Average loss: 2528.1217\n",
      "Train: [0/272 (0%)]\tLoss: 0.225592\n",
      "Train: [240/272 (88%)]\tLoss: 0.144387\n",
      "Epoch: 2/50. Train set: Average loss: 0.1403\n",
      "Epoch: 2/50. Validation set: Average loss: 236.3373\n",
      "Train: [0/272 (0%)]\tLoss: 0.048625\n",
      "Train: [240/272 (88%)]\tLoss: 0.056677\n",
      "Epoch: 3/50. Train set: Average loss: 0.0552\n",
      "Epoch: 3/50. Validation set: Average loss: 214.6242\n",
      "Train: [0/272 (0%)]\tLoss: 0.051783\n",
      "Train: [240/272 (88%)]\tLoss: 0.021570\n",
      "Epoch: 4/50. Train set: Average loss: 0.0218\n",
      "Epoch: 4/50. Validation set: Average loss: 71.8862\n",
      "Train: [0/272 (0%)]\tLoss: 0.008680\n",
      "Train: [240/272 (88%)]\tLoss: 0.010830\n",
      "Epoch: 5/50. Train set: Average loss: 0.0102\n",
      "Epoch: 5/50. Validation set: Average loss: 58.0367\n",
      "Train: [0/272 (0%)]\tLoss: 0.010867\n",
      "Train: [240/272 (88%)]\tLoss: 0.022442\n",
      "Epoch: 6/50. Train set: Average loss: 0.0212\n",
      "Epoch: 6/50. Validation set: Average loss: 338.0878\n",
      "Train: [0/272 (0%)]\tLoss: 0.015760\n",
      "Train: [240/272 (88%)]\tLoss: 0.018710\n",
      "Epoch: 7/50. Train set: Average loss: 0.0173\n",
      "Epoch: 7/50. Validation set: Average loss: 360.4860\n",
      "Train: [0/272 (0%)]\tLoss: 0.008174\n",
      "Train: [240/272 (88%)]\tLoss: 0.016862\n",
      "Epoch: 8/50. Train set: Average loss: 0.0154\n",
      "Epoch: 8/50. Validation set: Average loss: 134.1447\n",
      "Train: [0/272 (0%)]\tLoss: 0.003375\n",
      "Train: [240/272 (88%)]\tLoss: 0.040779\n",
      "Epoch: 9/50. Train set: Average loss: 0.0376\n",
      "Epoch: 9/50. Validation set: Average loss: 335.1503\n",
      "Train: [0/272 (0%)]\tLoss: 0.007864\n",
      "Train: [240/272 (88%)]\tLoss: 0.083151\n",
      "Epoch: 10/50. Train set: Average loss: 0.0749\n",
      "Epoch: 10/50. Validation set: Average loss: 188.6442\n",
      "Train: [0/272 (0%)]\tLoss: 0.016922\n",
      "Train: [240/272 (88%)]\tLoss: 0.010879\n",
      "Epoch: 11/50. Train set: Average loss: 0.0662\n",
      "Epoch: 11/50. Validation set: Average loss: 266.6395\n",
      "Train: [0/272 (0%)]\tLoss: 0.058007\n",
      "Train: [240/272 (88%)]\tLoss: 0.072536\n",
      "Epoch: 12/50. Train set: Average loss: 0.0660\n",
      "Epoch: 12/50. Validation set: Average loss: 54.2052\n",
      "Train: [0/272 (0%)]\tLoss: 0.002567\n",
      "Train: [240/272 (88%)]\tLoss: 0.007117\n",
      "Epoch: 13/50. Train set: Average loss: 0.0069\n",
      "Epoch: 13/50. Validation set: Average loss: 51.3715\n",
      "Train: [0/272 (0%)]\tLoss: 0.003302\n",
      "Train: [240/272 (88%)]\tLoss: 0.005742\n",
      "Epoch: 14/50. Train set: Average loss: 0.0054\n",
      "Epoch: 14/50. Validation set: Average loss: 41.2972\n",
      "Train: [0/272 (0%)]\tLoss: 0.002223\n",
      "Train: [240/272 (88%)]\tLoss: 0.003623\n",
      "Epoch: 15/50. Train set: Average loss: 0.0034\n",
      "Epoch: 15/50. Validation set: Average loss: 34.5769\n",
      "Train: [0/272 (0%)]\tLoss: 0.001307\n",
      "Train: [240/272 (88%)]\tLoss: 0.002845\n",
      "Epoch: 16/50. Train set: Average loss: 0.0028\n",
      "Epoch: 16/50. Validation set: Average loss: 31.1708\n",
      "Train: [0/272 (0%)]\tLoss: 0.001408\n",
      "Train: [240/272 (88%)]\tLoss: 0.002128\n",
      "Epoch: 17/50. Train set: Average loss: 0.0023\n",
      "Epoch: 17/50. Validation set: Average loss: 30.8614\n",
      "Train: [0/272 (0%)]\tLoss: 0.000903\n",
      "Train: [240/272 (88%)]\tLoss: 0.002984\n",
      "Epoch: 18/50. Train set: Average loss: 0.0028\n",
      "Epoch: 18/50. Validation set: Average loss: 30.4883\n",
      "Train: [0/272 (0%)]\tLoss: 0.001422\n",
      "Train: [240/272 (88%)]\tLoss: 0.002883\n",
      "Epoch: 19/50. Train set: Average loss: 0.0027\n",
      "Epoch: 19/50. Validation set: Average loss: 30.1789\n",
      "Train: [0/272 (0%)]\tLoss: 0.002006\n",
      "Train: [240/272 (88%)]\tLoss: 0.002295\n",
      "Epoch: 20/50. Train set: Average loss: 0.0023\n",
      "Epoch: 20/50. Validation set: Average loss: 29.7824\n",
      "Train: [0/272 (0%)]\tLoss: 0.001137\n",
      "Train: [240/272 (88%)]\tLoss: 0.002138\n",
      "Epoch: 21/50. Train set: Average loss: 0.0020\n",
      "Epoch: 21/50. Validation set: Average loss: 29.6044\n",
      "Train: [0/272 (0%)]\tLoss: 0.024208\n",
      "Train: [240/272 (88%)]\tLoss: 0.001825\n",
      "Epoch: 22/50. Train set: Average loss: 0.0026\n",
      "Epoch: 22/50. Validation set: Average loss: 29.3145\n",
      "Train: [0/272 (0%)]\tLoss: 0.001157\n",
      "Train: [240/272 (88%)]\tLoss: 0.002917\n",
      "Epoch: 23/50. Train set: Average loss: 0.0029\n",
      "Epoch: 23/50. Validation set: Average loss: 28.9557\n",
      "Train: [0/272 (0%)]\tLoss: 0.000617\n",
      "Train: [240/272 (88%)]\tLoss: 0.002513\n",
      "Epoch: 24/50. Train set: Average loss: 0.0024\n",
      "Epoch: 24/50. Validation set: Average loss: 28.6349\n",
      "Train: [0/272 (0%)]\tLoss: 0.001753\n",
      "Train: [240/272 (88%)]\tLoss: 0.002148\n",
      "Epoch: 25/50. Train set: Average loss: 0.0022\n",
      "Epoch: 25/50. Validation set: Average loss: 28.5988\n",
      "Train: [0/272 (0%)]\tLoss: 0.001258\n",
      "Train: [240/272 (88%)]\tLoss: 0.001759\n",
      "Epoch: 26/50. Train set: Average loss: 0.0016\n",
      "Epoch: 26/50. Validation set: Average loss: 28.5763\n",
      "Train: [0/272 (0%)]\tLoss: 0.002987\n",
      "Train: [240/272 (88%)]\tLoss: 0.001467\n",
      "Epoch: 27/50. Train set: Average loss: 0.0015\n",
      "Epoch: 27/50. Validation set: Average loss: 28.5577\n",
      "Train: [0/272 (0%)]\tLoss: 0.000977\n",
      "Train: [240/272 (88%)]\tLoss: 0.001762\n",
      "Epoch: 28/50. Train set: Average loss: 0.0020\n",
      "Epoch: 28/50. Validation set: Average loss: 28.5390\n",
      "Train: [0/272 (0%)]\tLoss: 0.001880\n",
      "Train: [240/272 (88%)]\tLoss: 0.002106\n",
      "Epoch: 29/50. Train set: Average loss: 0.0022\n",
      "Epoch: 29/50. Validation set: Average loss: 28.5075\n",
      "Train: [0/272 (0%)]\tLoss: 0.000614\n",
      "Train: [240/272 (88%)]\tLoss: 0.002569\n",
      "Epoch: 30/50. Train set: Average loss: 0.0024\n",
      "Epoch: 30/50. Validation set: Average loss: 28.4583\n",
      "Train: [0/272 (0%)]\tLoss: 0.001398\n",
      "Train: [240/272 (88%)]\tLoss: 0.002371\n",
      "Epoch: 31/50. Train set: Average loss: 0.0023\n",
      "Epoch: 31/50. Validation set: Average loss: 28.4189\n",
      "Train: [0/272 (0%)]\tLoss: 0.001586\n",
      "Train: [240/272 (88%)]\tLoss: 0.001696\n",
      "Epoch: 32/50. Train set: Average loss: 0.0016\n",
      "Epoch: 32/50. Validation set: Average loss: 28.4007\n",
      "Train: [0/272 (0%)]\tLoss: 0.001397\n",
      "Train: [240/272 (88%)]\tLoss: 0.002190\n",
      "Epoch: 33/50. Train set: Average loss: 0.0021\n",
      "Epoch: 33/50. Validation set: Average loss: 28.3984\n",
      "Train: [0/272 (0%)]\tLoss: 0.005076\n",
      "Train: [240/272 (88%)]\tLoss: 0.002213\n",
      "Epoch: 34/50. Train set: Average loss: 0.0023\n",
      "Epoch: 34/50. Validation set: Average loss: 28.3948\n",
      "Train: [0/272 (0%)]\tLoss: 0.000777\n",
      "Train: [240/272 (88%)]\tLoss: 0.001643\n",
      "Epoch: 35/50. Train set: Average loss: 0.0016\n",
      "Epoch: 35/50. Validation set: Average loss: 28.3925\n",
      "Train: [0/272 (0%)]\tLoss: 0.001019\n",
      "Train: [240/272 (88%)]\tLoss: 0.002279\n",
      "Epoch: 36/50. Train set: Average loss: 0.0021\n",
      "Epoch: 36/50. Validation set: Average loss: 28.3896\n",
      "Train: [0/272 (0%)]\tLoss: 0.001430\n",
      "Train: [240/272 (88%)]\tLoss: 0.001405\n",
      "Epoch: 37/50. Train set: Average loss: 0.0015\n",
      "Epoch: 37/50. Validation set: Average loss: 28.3881\n",
      "Train: [0/272 (0%)]\tLoss: 0.002816\n",
      "Train: [240/272 (88%)]\tLoss: 0.002325\n",
      "Epoch: 38/50. Train set: Average loss: 0.0023\n",
      "Epoch: 38/50. Validation set: Average loss: 28.3842\n",
      "Train: [0/272 (0%)]\tLoss: 0.001542\n",
      "Train: [240/272 (88%)]\tLoss: 0.001514\n",
      "Epoch: 39/50. Train set: Average loss: 0.0015\n",
      "Epoch: 39/50. Validation set: Average loss: 28.3819\n",
      "Train: [0/272 (0%)]\tLoss: 0.001487\n",
      "Train: [240/272 (88%)]\tLoss: 0.001565\n",
      "Epoch: 40/50. Train set: Average loss: 0.0015\n",
      "Epoch: 40/50. Validation set: Average loss: 28.3805\n",
      "Train: [0/272 (0%)]\tLoss: 0.001095\n",
      "Train: [240/272 (88%)]\tLoss: 0.001842\n",
      "Epoch: 41/50. Train set: Average loss: 0.0019\n",
      "Epoch: 41/50. Validation set: Average loss: 28.3805\n",
      "Train: [0/272 (0%)]\tLoss: 0.000570\n",
      "Train: [240/272 (88%)]\tLoss: 0.002308\n",
      "Epoch: 42/50. Train set: Average loss: 0.0023\n",
      "Epoch: 42/50. Validation set: Average loss: 28.3804\n",
      "Train: [0/272 (0%)]\tLoss: 0.001514\n",
      "Train: [240/272 (88%)]\tLoss: 0.001893\n",
      "Epoch: 43/50. Train set: Average loss: 0.0019\n",
      "Epoch: 43/50. Validation set: Average loss: 28.3804\n",
      "Train: [0/272 (0%)]\tLoss: 0.019649\n",
      "Train: [240/272 (88%)]\tLoss: 0.002145\n",
      "Epoch: 44/50. Train set: Average loss: 0.0027\n",
      "Epoch: 44/50. Validation set: Average loss: 28.3803\n",
      "Train: [0/272 (0%)]\tLoss: 0.001872\n",
      "Train: [240/272 (88%)]\tLoss: 0.002470\n",
      "Epoch: 45/50. Train set: Average loss: 0.0023\n",
      "Epoch: 45/50. Validation set: Average loss: 28.3802\n",
      "Train: [0/272 (0%)]\tLoss: 0.004336\n",
      "Train: [240/272 (88%)]\tLoss: 0.002107\n",
      "Epoch: 46/50. Train set: Average loss: 0.0022\n",
      "Epoch: 46/50. Validation set: Average loss: 28.3799\n",
      "Train: [0/272 (0%)]\tLoss: 0.003361\n",
      "Train: [240/272 (88%)]\tLoss: 0.001784\n",
      "Epoch: 47/50. Train set: Average loss: 0.0020\n",
      "Epoch: 47/50. Validation set: Average loss: 28.3799\n",
      "Train: [0/272 (0%)]\tLoss: 0.000822\n",
      "Train: [240/272 (88%)]\tLoss: 0.001786\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = fit(siamese_train_loader, siamese_test_loader, model, loss_fn, optimizer, scheduler, \n",
    "    n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, n_epochs), train_loss, 'rx-')\n",
    "plt.plot(range(0, n_epochs), val_loss, 'bx-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_cl, train_labels_cl = vis.extract_embeddings(train_loader, model)\n",
    "vis.plot_embeddings(train_embeddings_cl, train_labels_cl, siamese_train_dataset.labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings_baseline, val_labels_baseline = vis.extract_embeddings(test_loader, model)\n",
    "vis.plot_embeddings(val_embeddings_baseline, val_labels_baseline, siamese_test_dataset.labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients\n",
    "Test completeness axiom through comparison of different baselines\n",
    "\n",
    "\"Integrated gradients satisfy an\n",
    "axiom called completeness that the attributions add up to\n",
    "the difference between the output of F at the input x and\n",
    "the baseline x'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from captum.attr import LayerActivation\n",
    "from captum.attr import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = pd.read_csv(\"/srv/home/wconnell/keiser/data/uniprot_mapping_ids/map_ensembl_uniprot.csv\")\n",
    "reviewed_proteins = pd.read_csv(\"/srv/home/wconnell/keiser/data/uniprot_mapping_ids/TCGA_rnaseq_uniprot_features.tab.gz\", sep=\"\\t\")\n",
    "proteins = pd.merge(id_map, reviewed_proteins, left_on='UNIPROT_ID', right_on='Entry name')\n",
    "proteins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribution_pairs(SiameseTCGA, exp, ctrl):\n",
    "    # subset different samples\n",
    "    negative_pairs = np.array(SiameseTCGA.test_pairs)\n",
    "    negative_pairs = negative_pairs[negative_pairs[:,2] == 0]\n",
    "    # map labels to integers\n",
    "    ctrl = siamese_test_dataset.labels_dict[ctrl]\n",
    "    exp = siamese_test_dataset.labels_dict[exp]\n",
    "    # ordered indices of samples\n",
    "    ctrl_data = [idx for pair in negative_pairs[:, :2] for idx in pair if np.isin(idx, SiameseTCGA.label_to_indices[ctrl])]\n",
    "    exp_data = [idx for pair in negative_pairs[:, :2] for idx in pair if np.isin(idx, SiameseTCGA.label_to_indices[exp])]\n",
    "    # data\n",
    "    ctrl_data = Variable(SiameseTCGA.test_data[ctrl_data], requires_grad=True)\n",
    "    exp_data = Variable(SiameseTCGA.test_data[exp_data], requires_grad=True)\n",
    "    return ctrl_data, exp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IG with Control vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_data, exp_data = attribution_pairs(siamese_test_dataset, exp='Primary Tumor', ctrl='Solid Tissue Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(tmp_model.get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr, delta = ig.attribute(exp_data.cuda(), ctrl_data.cuda(), target=0, n_steps=50, return_convergence_delta=True)\n",
    "attr = attr.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr.shape\n",
    "cols = [ens[0] for ens in train_dataset.data.columns.str.split(\".\")]\n",
    "feat_imp = pd.DataFrame(data=attr, columns=cols, columns=['Attribution'])\n",
    "feat_imp = feat_imp[feat_imp.columns[np.isin(feat_imp.columns, proteins['ENSEMBL_ID'].values)]]\n",
    "feat_imp.hist(bins=100)\n",
    "feat_imp.describe()\n",
    "feat_imp.nlargest(10, columns='Attribution')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
