{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "                \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(12.7,10.27)})\n",
    "\n",
    "# notebook settings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTCGA(disease):\n",
    "    path = \"/srv/nas/mk2/projects/pan-cancer/TCGA_CCLE_GCP/TCGA/TCGA_{}_counts.tsv.gz\"\n",
    "    files = [path.format(d) for d in disease]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGCP(files):\n",
    "    \"\"\"\n",
    "    Paths to count matrices.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    for f in files:\n",
    "        key = os.path.basename(f).split(\"_\")[1]\n",
    "        data = pd.read_csv(f, sep='\\t', index_col=0)\n",
    "        meta = pd.DataFrame([row[:-1] for row in data.index.str.split(\"|\")],\n",
    "                            columns=['ENST', 'ENSG', 'OTTHUMG', 'OTTHUMT', 'GENE-NUM', 'GENE', 'NUM', 'TYPE'])\n",
    "        data.index = meta['GENE']\n",
    "        data_dict[key] = data.T\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameTCGA(data_dict, mapper):\n",
    "    for key in data_dict.keys():\n",
    "        data_dict[key] = data_dict[key].rename(mapper)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uq_norm(df, q=0.75):\n",
    "    \"\"\"\n",
    "    Upper quartile normalization of GEX for samples.\n",
    "    \"\"\"\n",
    "    quantiles = df.quantile(q=q, axis=1)\n",
    "    norm = df.divide(quantiles, axis=0)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4e86a7e95e19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtcga_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTCGA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisease\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtcga_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TCGA/TCGA_GDC_ID_MAP.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtcga_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadGCP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtcga_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-ba318c709b52>\u001b[0m in \u001b[0;36mreadGCP\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         meta = pd.DataFrame([row[:-1] for row in data.index.str.split(\"|\")],\n\u001b[1;32m     10\u001b[0m                             columns=['ENST', 'ENSG', 'OTTHUMG', 'OTTHUMT', 'GENE-NUM', 'GENE', 'NUM', 'TYPE'])\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m     \"\"\"\n\u001b[1;32m    680\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base = \"/srv/nas/mk2/projects/pan-cancer/TCGA_CCLE_GCP\"\n",
    "disease = ['BRCA', 'LUAD', 'KIRC', 'THCA', 'PRAD', 'SKCM']\n",
    "\n",
    "tcga_files = getTCGA(disease)\n",
    "tcga_meta = pd.read_csv(os.path.join(base, \"TCGA/TCGA_GDC_ID_MAP.tsv\"), sep=\"\\t\")\n",
    "tcga_raw = readGCP(tcga_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename samples to reflect canonical IDs\n",
    "tcga_raw = renameTCGA(tcga_raw, mapper=dict(zip(tcga_meta['CGHubAnalysisID'], tcga_meta['Sample ID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine samples\n",
    "tcga_raw = pd.concat(tcga_raw.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper quartile normalization\n",
    "tcga_raw = uq_norm(tcga_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log norm\n",
    "tcga = tcga_raw.transform(np.log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample\n",
    "tcga = tcga.sample(n=18000, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_meta[tcga_meta['Sample ID'] == 'TCGA-A7-A26F-01B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict \n",
    "hierarchy = OrderedDict({'Disease':['BRCA', 'LUAD', 'KIRC', 'THCA', 'PRAD', 'SKCM'],\n",
    "                         'Sample Type':['Primary Tumor', 'Solid Tissue Normal', 'Metastatic']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment():\n",
    "    \"\"\"\n",
    "    Defines an experimental class hierarchy object.\n",
    "    \"\"\"\n",
    "    def __init__(self, meta_data, hierarchy, cases, min_samples):\n",
    "        self.hierarchy = hierarchy\n",
    "        self.meta_data = self.categorize(meta_data, self.hierarchy, min_samples)\n",
    "        self.cases = self.meta_data[cases].unique()\n",
    "        self.labels = self.meta_data['meta'].cat.codes.values.astype('int')\n",
    "        self.labels_dict = {key:val for key,val in enumerate(self.meta_data['meta'].cat.categories.values)}\n",
    "        \n",
    "    def categorize(self, meta_data, hierarchy, min_samples):\n",
    "        assert isinstance(hierarchy, OrderedDict), \"Argument of wrong type.\"\n",
    "        # downsample data\n",
    "        for key,val in hierarchy.items():\n",
    "            meta_data = meta_data[meta_data[key].isin(val)]\n",
    "        # unique meta classes\n",
    "        meta_data['meta'] = meta_data[list(hierarchy.keys())].apply(lambda row: ':'.join(row.values.astype(str)), axis=1)\n",
    "        # filter meta classes\n",
    "        counts = meta_data['meta'].value_counts()\n",
    "        keep = counts[counts > min_samples].index\n",
    "        meta_data = meta_data[meta_data['meta'].isin(keep)]\n",
    "        # generate class categories\n",
    "        meta_data['meta'] = meta_data['meta'].astype('category')\n",
    "        return meta_data\n",
    "    \n",
    "    def holdout(self, holdout):\n",
    "        self.holdout = holdout\n",
    "        self.holdout_samples = self.meta_data[self.meta_data['meta'].isin(holdout)]\n",
    "        self.meta_data = self.meta_data[~self.meta_data['meta'].isin(holdout)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dutils import train_test_split_case\n",
    "exp = Experiment(meta_data=tcga_meta,\n",
    "                 hierarchy=hierarchy,\n",
    "                 cases='Case ID',\n",
    "                 min_samples=20)\n",
    "exp.holdout(holdout=['SKCM:Metastatic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.meta_data['meta'].value_counts()\n",
    "exp.holdout_samples['meta'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Train / Test sample split\n",
    "target = 'meta'\n",
    "\n",
    "train, test = train_test_split_case(exp.meta_data, cases='Case ID')\n",
    "# stratification is not quite perfect but close\n",
    "# in order to preserve matched samples for each case together\n",
    "# in train or test set\n",
    "case_counts = exp.meta_data[target].value_counts()\n",
    "train[target].value_counts()[case_counts.index.to_numpy()] / case_counts\n",
    "test[target].value_counts()[case_counts.index.to_numpy()] / case_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_data = tcga[tcga.index.isin(train['Sample ID'])].astype(np.float16)\n",
    "test_data = tcga[tcga.index.isin(test['Sample ID'])].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "#torch.manual_seed(123)\n",
    "\n",
    "from trainer import fit\n",
    "import visualization as vis\n",
    "import numpy as np\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"Cuda is available: {}\".format(cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Train: For each sample creates randomly a positive or a negative pair\n",
    "    Test: Creates fixed pairs for testing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, experiment, data, train=False):\n",
    "        self.train = train\n",
    "        self.labels = experiment.meta_data[experiment\n",
    "                                           .meta_data['Sample ID']\n",
    "                                           .isin(data.index)]['meta'].cat.codes.values.astype('int')\n",
    "        assert len(data) == len(self.labels)\n",
    "\n",
    "        if self.train:\n",
    "            self.train_labels = self.labels\n",
    "            self.train_data = torch.from_numpy(data.values).float()\n",
    "            self.labels_set = set(self.train_labels)\n",
    "            self.label_to_indices = {label: np.where(self.train_labels == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "        else:\n",
    "            # generate fixed pairs for testing\n",
    "            self.test_labels = self.labels\n",
    "            self.test_data = torch.from_numpy(data.values).float()\n",
    "            self.labels_set = set(self.test_labels)\n",
    "            self.label_to_indices = {label: np.where(self.test_labels == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "            random_state = np.random.RandomState(29)\n",
    "\n",
    "            positive_pairs = [[i,\n",
    "                               random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
    "                               1]\n",
    "                              for i in range(0, len(self.test_data), 2)]\n",
    "\n",
    "            negative_pairs = [[i,\n",
    "                               random_state.choice(self.label_to_indices[\n",
    "                                                       np.random.choice(\n",
    "                                                           list(self.labels_set - set([self.test_labels[i].item()]))\n",
    "                                                       )\n",
    "                                                   ]),\n",
    "                               0]\n",
    "                              for i in range(1, len(self.test_data), 2)]\n",
    "            self.test_pairs = positive_pairs + negative_pairs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            target = np.random.randint(0, 2)\n",
    "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
    "            if target == 1:\n",
    "                siamese_index = index\n",
    "                while siamese_index == index:\n",
    "                    siamese_index = np.random.choice(self.label_to_indices[label1])\n",
    "            else:\n",
    "                siamese_label = np.random.choice(list(self.labels_set - set([label1])))\n",
    "                siamese_index = np.random.choice(self.label_to_indices[siamese_label])\n",
    "            img2 = self.train_data[siamese_index]\n",
    "        else:\n",
    "            img1 = self.test_data[self.test_pairs[index][0]]\n",
    "            img2 = self.test_data[self.test_pairs[index][1]]\n",
    "            target = self.test_pairs[index][2]\n",
    "        \n",
    "        return (img1, img2), target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_train_dataset = SiameseDataset(experiment=exp,\n",
    "                                       data=train_data,\n",
    "                                       train=True)\n",
    "siamese_test_dataset = SiameseDataset(experiment=exp,\n",
    "                                       data=test_data,\n",
    "                                       train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_test_dataset.test_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "kwargs = {'num_workers': 10, 'pin_memory': True} if cuda else {'num_workers': 10}\n",
    "siamese_train_loader = torch.utils.data.DataLoader(siamese_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "siamese_test_loader = torch.utils.data.DataLoader(siamese_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "from tcga_networks import EmbeddingNet, SiameseNet\n",
    "from losses import ContrastiveLoss, TripletLoss\n",
    "from metrics import AccumulatedAccuracyMetric\n",
    "\n",
    "# Step 2\n",
    "n_samples, n_features = siamese_train_dataset.train_data.shape\n",
    "embedding_net = EmbeddingNet(n_features, 2)\n",
    "# Step 3\n",
    "model = SiameseNet(embedding_net)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "# Step 4\n",
    "margin = 1.\n",
    "loss_fn = ContrastiveLoss(margin)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 10\n",
    "# print training metrics every log_interval * batch_size\n",
    "log_interval = round(len(siamese_train_dataset)/4/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = fit(siamese_train_loader, siamese_test_loader, model, loss_fn, optimizer, scheduler, \n",
    "    n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, n_epochs), train_loss, 'rx-', label='train')\n",
    "plt.plot(range(0, n_epochs), val_loss, 'bx-', label='validation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def extract_embeddings(samples, target, model):\n",
    "    cuda = torch.cuda.is_available()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        assert len(samples) == len(target)\n",
    "        embeddings = np.zeros((len(samples), 2))\n",
    "        labels = np.zeros(len(target))\n",
    "        k = 0\n",
    "        if cuda:\n",
    "            samples = samples.cuda()\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            embeddings[k:k+len(samples)] = model.module.get_embedding(samples).data.cpu().numpy()\n",
    "        else:\n",
    "            embeddings[k:k+len(samples)] = model.get_embedding(samples).data.cpu().numpy()\n",
    "        labels[k:k+len(samples)] = target\n",
    "        k += len(samples)\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_cl, train_labels_cl = extract_embeddings(siamese_train_dataset.train_data, siamese_train_dataset.labels, model)\n",
    "vis.sns_plot_embeddings(train_embeddings_cl, train_labels_cl, exp.labels_dict, \n",
    "                        hue='meta', style='Sample Type', alpha=0.5)\n",
    "plt.title('PanCancer Train: Siamese')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings_baseline, val_labels_baseline = extract_embeddings(siamese_test_dataset.test_data, siamese_test_dataset.labels, model)\n",
    "vis.sns_plot_embeddings(val_embeddings_baseline, val_labels_baseline, exp.labels_dict, \n",
    "                        hue='meta', style='Sample Type', alpha=0.5)\n",
    "plt.title('PanCancer Test: Siamese')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
