{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import datetime\n",
    "import pathlib\n",
    "\n",
    "from collections import OrderedDict \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Custom\n",
    "from dutils import Experiment\n",
    "from trainer import fit\n",
    "import visualization as vis\n",
    "from tcga_datasets import SiameseDataset\n",
    "\n",
    "# Models\n",
    "from tcga_networks import EmbeddingNet, SiameseNet\n",
    "from losses import ContrastiveLoss\n",
    "\n",
    "# Metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_mutual_info_score as ANMI\n",
    "\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTCGA(disease):\n",
    "    path = \"/srv/nas/mk2/projects/pan-cancer/TCGA_CCLE_GCP/TCGA/TCGA_{}_counts.tsv.gz\"\n",
    "    files = [path.format(d) for d in disease]\n",
    "    return files\n",
    "\n",
    "\n",
    "def readGCP(files, biotype='protein_coding', mean=True):\n",
    "    \"\"\"\n",
    "    Paths to count matrices.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    for f in files:\n",
    "        key = os.path.basename(f).split(\"_\")[1]\n",
    "        data = pd.read_csv(f, sep='\\t', index_col=0)\n",
    "        # transcript metadata\n",
    "        meta = pd.DataFrame([row[:-1] for row in data.index.str.split(\"|\")],\n",
    "                            columns=['ENST', 'ENSG', 'OTTHUMG', 'OTTHUMT', 'GENE-NUM', 'GENE', 'BP', 'BIOTYPE'])\n",
    "        meta = pd.MultiIndex.from_frame(meta)\n",
    "        data.index = meta\n",
    "        # subset transcripts\n",
    "        data = data.xs(key=biotype, level='BIOTYPE')\n",
    "        data = data.droplevel(['ENST', 'ENSG', 'OTTHUMG', 'OTTHUMT', 'GENE-NUM', 'BP'])\n",
    "        # average gene expression of splice variants\n",
    "        data = data.T\n",
    "        if mean:\n",
    "            data = data.groupby(by=data.columns, axis=1).mean()\n",
    "        data_dict[key] = data\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def uq_norm(df, q=0.75):\n",
    "    \"\"\"\n",
    "    Upper quartile normalization of GEX for samples.\n",
    "    \"\"\"\n",
    "    quantiles = df.quantile(q=q, axis=1)\n",
    "    norm = df.divide(quantiles, axis=0)\n",
    "    return norm\n",
    "\n",
    "\n",
    "def process_TCGA(disease=['BRCA', 'LUAD', 'KIRC', 'THCA', 'PRAD', 'SKCM']):\n",
    "    base=\"/srv/nas/mk2/projects/pan-cancer/TCGA_CCLE_GCP\"\n",
    "    # get files\n",
    "    tcga_files = getTCGA(disease)\n",
    "    # read meta/data\n",
    "    tcga_meta = pd.read_csv(os.path.join(base, \"TCGA/TCGA_GDC_ID_MAP.tsv\"), sep=\"\\t\")\n",
    "    tcga_raw = readGCP(tcga_files, mean=True)\n",
    "    # combine samples\n",
    "    tcga_raw = pd.concat(tcga_raw.values())\n",
    "    # Upper quartile normalization\n",
    "    tcga_raw = uq_norm(tcga_raw)\n",
    "    # log norm\n",
    "    tcga = tcga_raw.transform(np.log1p)\n",
    "    return tcga, tcga_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fsets(data, n_features, steps=5):\n",
    "    r = np.linspace(start=1, stop=n_features, num=steps, dtype='int')\n",
    "    idx = [np.random.randint(low=1, high=data.shape[1], size=i) for i in r]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(n_features, embedding_dim, margin, lr, device):\n",
    "    embedding_net = EmbeddingNet(n_features, embedding_dim)\n",
    "    model = SiameseNet(embedding_net)\n",
    "    model.cuda(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_eval(test_embeddings, test_labels, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    siamese_clusters = kmeans.fit_predict(test_embeddings)\n",
    "    anmi = ANMI(siamese_clusters, test_labels)\n",
    "    return anmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_training(train_data, train_labels, test_data, test_labels, \n",
    "                     feature_idx, exp_dir, \n",
    "                     devices, embedding_dim, n_epochs, cuda=True):\n",
    "    # Meta data\n",
    "    meta_data = {\"n_features\":[],\n",
    "                 \"model\":[],\n",
    "                 \"ANMI\":[]}\n",
    "    # Params\n",
    "    batch_size = 8\n",
    "    kwargs = {'num_workers': 10, 'pin_memory': True} if cuda else {'num_workers': 10}\n",
    "    \n",
    "    # Feature Index\n",
    "    for batch, feat in enumerate(feature_idx):\n",
    "        print(\"Number features: {}\\n\".format(len(feat)))\n",
    "        exp_data = {'feature_idx':feat}\n",
    "        # Define data\n",
    "        siamese_train_dataset = SiameseDataset(data=train_data.iloc[:,feat],\n",
    "                                           labels=train_labels,\n",
    "                                           train=True)\n",
    "        siamese_test_dataset = SiameseDataset(data=test_data.iloc[:,feat],\n",
    "                                          labels=test_labels,\n",
    "                                          train=False)\n",
    "        # Loaders\n",
    "        siamese_train_loader = torch.utils.data.DataLoader(siamese_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "        siamese_test_loader = torch.utils.data.DataLoader(siamese_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        # Instantiate model\n",
    "        n_samples, n_features = siamese_train_dataset.train_data.shape\n",
    "        # Parameters\n",
    "        margin = 1.\n",
    "        lr = 1e-3\n",
    "        loss_fn = ContrastiveLoss(margin)\n",
    "        log_interval = round(len(siamese_train_dataset)/1/batch_size)\n",
    "        \n",
    "        if cuda:\n",
    "            models = [init_model(n_features,\n",
    "                                 embedding_dim,\n",
    "                                 margin=margin, \n",
    "                                 lr=lr, \n",
    "                                 device=d) for d in devices]\n",
    "        # Training\n",
    "        dlosses = [dask.delayed(fit)(siamese_train_loader, \n",
    "                                    siamese_test_loader, \n",
    "                                    model, \n",
    "                                    loss_fn, \n",
    "                                    optimizer, \n",
    "                                    scheduler, \n",
    "                                    n_epochs, \n",
    "                                    cuda, \n",
    "                                    log_interval) for model,optimizer,scheduler in models]\n",
    "        losses = dask.compute(*dlosses)\n",
    "        print('Done with losses')\n",
    "        # Test Embeddings\n",
    "        dembeddings = [dask.delayed(vis.extract_embeddings)(siamese_test_dataset.test_data, \n",
    "                                                            siamese_test_dataset.labels, \n",
    "                                                            model) for model,optimizer,scheduler in models]\n",
    "        embeddings = dask.compute(*dembeddings)\n",
    "        print('Done with embeddings')\n",
    "        # Evaluation\n",
    "        anmi_eval = [dask.delayed(cluster_eval)(test_embeddings, \n",
    "                                                test_labels, \n",
    "                                                len(np.unique(test_labels))) for test_embeddings,test_labels in embeddings]\n",
    "        print('Done with eval')\n",
    "        # Store\n",
    "        for i,anmi in enumerate(anmi_eval):\n",
    "            nmodel = 'model_{}'.format(i)\n",
    "            meta_data['n_features'].append(len(feat))\n",
    "            meta_data['model'].append(nmodel)\n",
    "            meta_data['ANMI'].append(anmi)\n",
    "\n",
    "        exp_data[nmodel] = {'data': embeddings,\n",
    "                            'loss': losses,\n",
    "                            'ANMI': anmi_eval}\n",
    "        pd.to_pickle(exp_data, os.path.join(exp_dir, \"model_{}.pkl\".format(len(feat))))\n",
    "        \n",
    "    pd.to_pickle(meta_data, os.path.join(exp_dir, \"model_meta_data.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(disease, sample_type, **kwargs):\n",
    "    # GPUs\n",
    "    cuda = torch.cuda.is_available()\n",
    "    print(\"Cuda is available: {}\".format(cuda))\n",
    "    \n",
    "    # Read / write / process\n",
    "    tcga, tcga_meta = process_TCGA(disease)\n",
    "    # Feature design\n",
    "    feature_idx = generate_fsets(tcga, n_features=kwargs['n_features'], steps=kwargs['steps'])\n",
    "    # Experiment design\n",
    "    hierarchy = OrderedDict({'Disease':disease,\n",
    "                             'Sample Type':sample_type})\n",
    "    # Define experiment\n",
    "    exp = Experiment(meta_data=tcga_meta,\n",
    "                     hierarchy=hierarchy,\n",
    "                     index='CGHubAnalysisID',\n",
    "                     cases='Case ID',\n",
    "                     min_samples=20)\n",
    "    # Train / Test split\n",
    "    exp.train_test_split(cases='Case ID')\n",
    "    # Return data \n",
    "    train_data, train_labels = exp.get_data(tcga, subset=\"train\", dtype=np.float32)\n",
    "    test_data, test_labels = exp.get_data(tcga, subset=\"test\", dtype=np.float32)\n",
    "    \n",
    "    # Path *fix*\n",
    "    dtime = datetime.datetime.today().strftime(\"%Y.%m.%d_%H:%M\")\n",
    "    exp_dir = \"/srv/nas/mk2/projects/pan-cancer/experiments/test/{}_{}_{}_{}-{}\".format(dtime, \n",
    "                                                                                len(exp.labels_dict),\n",
    "                                                                                kwargs['embedding'],\n",
    "                                                                                kwargs['n_features'], \n",
    "                                                                                kwargs['steps'])\n",
    "    pathlib.Path(exp_dir).mkdir(parents=True, exist_ok=False)\n",
    "    print('Saving to: \\n{}'.format(exp_dir))\n",
    "    \n",
    "    # Meta data\n",
    "    experiments = {'experiment': exp,\n",
    "                   'train':(train_data, train_labels),\n",
    "                   'test': (test_data, test_labels)}\n",
    "    pd.to_pickle(experiments, os.path.join(exp_dir, \"experiment_meta_data.pkl\"))\n",
    "    \n",
    "    # Training\n",
    "    feature_training(train_data, train_labels, test_data, test_labels, \n",
    "                     feature_idx, exp_dir, \n",
    "                     kwargs['devices'], kwargs['embedding'], kwargs['n_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base=\"/srv/nas/mk2/projects/pan-cancer/TCGA_CCLE_GCP\"\n",
    "# read meta/data\n",
    "tcga_meta = pd.read_csv(os.path.join(base, \"TCGA/TCGA_GDC_ID_MAP.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HNSC']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disease = tcga_meta[tcga_meta['Sample Type']=='Solid Tissue Normal']['Disease'].value_counts()\n",
    "disease = list(disease[disease>=20].index)[7:8]\n",
    "disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_type = ['Primary Tumor', 'Solid Tissue Normal']\n",
    "params = {\"devices\":[4,5,6],\n",
    "          \"n_features\":2000,\n",
    "          \"steps\":3,\n",
    "          \"embedding\":2,\n",
    "          \"n_epochs\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: True\n",
      "Saving to: \n",
      "/srv/nas/mk2/projects/pan-cancer/experiments/test/2020.03.27_16:32_2_2_2000-3\n",
      "Number features: 1\n",
      "\n",
      "Train: [0/437 (0%)]\tLoss: 0.124892\n",
      "Train: [0/437 (0%)]\tLoss: 0.124759\n",
      "Train: [0/437 (0%)]\tLoss: 0.124716\n",
      "Epoch: 1/3. Train set: Average loss: 0.1675\n",
      "Epoch: 1/3. Validation set: Average loss: 0.1651\n",
      "Epoch: 1/3. Train set: Average loss: 0.1706\n",
      "Epoch: 1/3. Validation set: Average loss: 0.1888\n",
      "Epoch: 1/3. Train set: Average loss: 0.1714\n",
      "Epoch: 1/3. Validation set: Average loss: 0.1744\n",
      "Train: [0/437 (0%)]\tLoss: 0.100629\n",
      "Train: [0/437 (0%)]\tLoss: 0.130892\n",
      "Train: [0/437 (0%)]\tLoss: 0.118239\n",
      "Epoch: 1/3. Train set: Average loss: 0.1714\n",
      "Epoch: 1/3. Validation set: Average loss: 0.1744\n",
      "Epoch: 2/3. Train set: Average loss: 0.1695\n",
      "Epoch: 2/3. Validation set: Average loss: 0.1616\n",
      "Epoch: 2/3. Train set: Average loss: 0.1705\n",
      "Epoch: 2/3. Validation set: Average loss: 0.1734\n",
      "Epoch: 2/3. Train set: Average loss: 0.1847\n",
      "Epoch: 2/3. Validation set: Average loss: 0.1712\n",
      "Train: [0/437 (0%)]\tLoss: 0.131267\n",
      "Train: [0/437 (0%)]\tLoss: 0.062830\n",
      "Train: [0/437 (0%)]\tLoss: 0.226560\n",
      "Epoch: 2/3. Train set: Average loss: 0.1847\n",
      "Epoch: 2/3. Validation set: Average loss: 0.1712\n",
      "Epoch: 3/3. Train set: Average loss: 0.1826\n",
      "Epoch: 3/3. Validation set: Average loss: 0.1709\n",
      "Epoch: 3/3. Train set: Average loss: 0.1645\n",
      "Epoch: 3/3. Validation set: Average loss: 0.1610\n",
      "Epoch: 3/3. Train set: Average loss: 0.1840\n",
      "Epoch: 3/3. Validation set: Average loss: 0.1608\n",
      "Done with losses\n",
      "Done with embeddings\n",
      "Done with eval\n",
      "Number features: 1000\n",
      "\n",
      "Train: [0/437 (0%)]\tLoss: 0.124497\n",
      "Train: [0/437 (0%)]\tLoss: 0.124660\n",
      "Train: [0/437 (0%)]\tLoss: 0.124502\n",
      "Epoch: 1/3. Train set: Average loss: 0.1134\n",
      "Epoch: 1/3. Validation set: Average loss: 0.0809\n",
      "Epoch: 1/3. Train set: Average loss: 0.0361\n",
      "Epoch: 1/3. Validation set: Average loss: 0.0297\n",
      "Epoch: 1/3. Train set: Average loss: 0.0404\n",
      "Epoch: 1/3. Validation set: Average loss: 0.0337\n",
      "Train: [0/437 (0%)]\tLoss: 0.024255\n",
      "Train: [0/437 (0%)]\tLoss: 0.114062\n",
      "Train: [0/437 (0%)]\tLoss: 0.029948\n",
      "Epoch: 1/3. Train set: Average loss: 0.0404\n",
      "Epoch: 1/3. Validation set: Average loss: 0.0337\n",
      "Epoch: 2/3. Train set: Average loss: 0.0199\n",
      "Epoch: 2/3. Validation set: Average loss: 0.0270\n",
      "Epoch: 2/3. Train set: Average loss: 0.0191\n",
      "Epoch: 2/3. Validation set: Average loss: 0.0260\n",
      "Epoch: 2/3. Train set: Average loss: 0.0597\n",
      "Epoch: 2/3. Validation set: Average loss: 0.0601\n",
      "Train: [0/437 (0%)]\tLoss: 0.021895\n",
      "Train: [0/437 (0%)]\tLoss: 0.007579\n",
      "Train: [0/437 (0%)]\tLoss: 0.019434\n",
      "Epoch: 3/3. Train set: Average loss: 0.0600\n",
      "Epoch: 3/3. Validation set: Average loss: 0.0389\n",
      "Epoch: 3/3. Train set: Average loss: 0.0168\n",
      "Epoch: 3/3. Validation set: Average loss: 0.0148\n",
      "Epoch: 3/3. Train set: Average loss: 0.0179\n",
      "Epoch: 3/3. Validation set: Average loss: 0.0268\n",
      "Done with losses\n",
      "Done with embeddings\n",
      "Done with eval\n",
      "Number features: 2000\n",
      "\n",
      "Train: [0/437 (0%)]\tLoss: 0.124531\n",
      "Train: [0/437 (0%)]\tLoss: 0.124783\n",
      "Train: [0/437 (0%)]\tLoss: 0.124423\n",
      "Epoch: 1/3. Train set: Average loss: 0.1032\n",
      "Epoch: 1/3. Validation set: Average loss: 0.0615\n",
      "Epoch: 1/3. Train set: Average loss: 0.0784\n",
      "Epoch: 1/3. Validation set: Average loss: 0.0437\n",
      "Epoch: 1/3. Train set: Average loss: 0.0700\n",
      "Epoch: 1/3. Validation set: Average loss: 0.0736\n",
      "Train: [0/437 (0%)]\tLoss: 0.088429\n",
      "Train: [0/437 (0%)]\tLoss: 0.050366\n",
      "Train: [0/437 (0%)]\tLoss: 0.051784\n",
      "Epoch: 1/3. Train set: Average loss: 0.0784\n",
      "Epoch: 1/3. Validation set: Average loss: 0.0437\n",
      "Epoch: 2/3. Train set: Average loss: 0.0613\n",
      "Epoch: 2/3. Validation set: Average loss: 0.0403\n",
      "Epoch: 2/3. Train set: Average loss: 0.0255\n",
      "Epoch: 2/3. Validation set: Average loss: 0.0204Epoch: 2/3. Train set: Average loss: 0.0236\n",
      "Epoch: 2/3. Validation set: Average loss: 0.1260\n",
      "\n",
      "Train: [0/437 (0%)]\tLoss: 0.025300\n",
      "Train: [0/437 (0%)]\tLoss: 0.029184\n",
      "Train: [0/437 (0%)]\tLoss: 0.033644\n",
      "Epoch: 2/3. Train set: Average loss: 0.0255\n",
      "Epoch: 2/3. Validation set: Average loss: 0.0204Epoch: 2/3. Train set: Average loss: 0.0236\n",
      "Epoch: 2/3. Validation set: Average loss: 0.1260\n",
      "Epoch: 3/3. Train set: Average loss: 0.0236\n",
      "Epoch: 3/3. Validation set: Average loss: 0.0298\n",
      "Epoch: 3/3. Train set: Average loss: 0.0232\n",
      "Epoch: 3/3. Validation set: Average loss: 0.0222\n",
      "Epoch: 3/3. Train set: Average loss: 0.0184\n",
      "Epoch: 3/3. Validation set: Average loss: 0.0212\n",
      "Done with losses\n",
      "Done with embeddings\n",
      "Done with eval\n"
     ]
    }
   ],
   "source": [
    "main(disease=disease, sample_type=sample_type, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
