{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from collections import OrderedDict \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(12.7,10.27)})\n",
    "\n",
    "# notebook settings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "#torch.manual_seed(123)\n",
    "\n",
    "from dutils import Experiment\n",
    "from trainer import fit\n",
    "import visualization as vis\n",
    "from tcga_datasets import SiameseDataset\n",
    "import numpy as np\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"Cuda is available: {}\".format(cuda))\n",
    "\n",
    "# Models\n",
    "from tcga_networks import EmbeddingNet, SiameseNet\n",
    "from losses import ContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_mutual_info_score as ANMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTCGA(disease):\n",
    "    path = \"/srv/nas/mk2/projects/pan-cancer/TCGA_CCLE_GCP/TCGA/TCGA_{}_counts.tsv.gz\"\n",
    "    files = [path.format(d) for d in disease]\n",
    "    return files\n",
    "\n",
    "\n",
    "def readGCP(files, biotype='protein_coding', mean=True):\n",
    "    \"\"\"\n",
    "    Paths to count matrices.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    for f in files:\n",
    "        key = os.path.basename(f).split(\"_\")[1]\n",
    "        data = pd.read_csv(f, sep='\\t', index_col=0)\n",
    "        # transcript metadata\n",
    "        meta = pd.DataFrame([row[:-1] for row in data.index.str.split(\"|\")],\n",
    "                            columns=['ENST', 'ENSG', 'OTTHUMG', 'OTTHUMT', 'GENE-NUM', 'GENE', 'BP', 'BIOTYPE'])\n",
    "        meta = pd.MultiIndex.from_frame(meta)\n",
    "        data.index = meta\n",
    "        # subset transcripts\n",
    "        data = data.xs(key=biotype, level='BIOTYPE')\n",
    "        data = data.droplevel(['ENST', 'ENSG', 'OTTHUMG', 'OTTHUMT', 'GENE-NUM', 'BP'])\n",
    "        # average gene expression of splice variants\n",
    "        data = data.T\n",
    "        if mean:\n",
    "            data = data.groupby(by=data.columns, axis=1).mean()\n",
    "        data_dict[key] = data\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def uq_norm(df, q=0.75):\n",
    "    \"\"\"\n",
    "    Upper quartile normalization of GEX for samples.\n",
    "    \"\"\"\n",
    "    quantiles = df.quantile(q=q, axis=1)\n",
    "    norm = df.divide(quantiles, axis=0)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fsets(data, steps=5):\n",
    "    n = np.linspace(start=1, stop=data.shape[1], num=steps, dtype='int')\n",
    "    idx = [np.random.randint(low=1, high=data.shape[1], size=i) for i in n]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    base = \"/srv/nas/mk2/projects/pan-cancer/TCGA_CCLE_GCP\"\n",
    "    disease = ['BRCA', 'LUAD', 'KIRC', 'THCA', 'PRAD', 'SKCM']\n",
    "\n",
    "    tcga_files = getTCGA(disease)\n",
    "    tcga_meta = pd.read_csv(os.path.join(base, \"TCGA/TCGA_GDC_ID_MAP.tsv\"), sep=\"\\t\")\n",
    "    tcga_raw = readGCP(tcga_files, mean=True)\n",
    "    # combine samples\n",
    "    tcga_raw = pd.concat(tcga_raw.values())\n",
    "    # Upper quartile normalization\n",
    "    tcga_raw = uq_norm(tcga_raw)\n",
    "    # log norm\n",
    "    tcga = tcga_raw.transform(np.log1p)\n",
    "    return tcga, tcga_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = \"/srv/nas/mk2/projects/pan-cancer/experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga, tcga_meta = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idx = generate_fsets(tcga, steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy = OrderedDict({'Disease':['BRCA', 'LUAD', 'KIRC', 'THCA', 'PRAD', 'SKCM'],\n",
    "                         'Sample Type':['Primary Tumor', 'Solid Tissue Normal']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment\n",
    "exp = Experiment(meta_data=tcga_meta,\n",
    "                 hierarchy=hierarchy,\n",
    "                 index='CGHubAnalysisID',\n",
    "                 cases='Case ID',\n",
    "                 min_samples=20)\n",
    "# Train / Test split\n",
    "exp.train_test_split(cases='Case ID')\n",
    "# Return data \n",
    "train_data, train_labels = exp.get_data(tcga, subset=\"train\", dtype=np.float32)\n",
    "test_data, test_labels = exp.get_data(tcga, subset=\"test\", dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {'experiment': exp,\n",
    "               'train':(train_data, train_labels),\n",
    "               'test': (test_data, test_labels)}\n",
    "pd.to_pickle(experiments, os.path.join(exp_dir, \"experiment_meta_data.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_training(train_data, train_labels, test_data, test_labels, feature_idx, exp_dir, cuda=True):\n",
    "    # Meta data\n",
    "    meta_data = {\"n_features\":[],\n",
    "                 \"model\":[],\n",
    "                 \"ANMI\":[]}\n",
    "    # Params\n",
    "    batch_size = 8\n",
    "    kwargs = {'num_workers': 10, 'pin_memory': True} if cuda else {'num_workers': 10}\n",
    "    \n",
    "    # Feature Index\n",
    "    for batch, feat in enumerate(feature_idx):\n",
    "        print(\"Batch {}\\n\".format(batch))\n",
    "        exp_data = {'feature_idx':feat}\n",
    "        # Define data\n",
    "        siamese_train_dataset = SiameseDataset(data=train_data.iloc[:,feat],\n",
    "                                           labels=train_labels,\n",
    "                                           train=True)\n",
    "        siamese_test_dataset = SiameseDataset(data=test_data.iloc[:,feat],\n",
    "                                          labels=test_labels,\n",
    "                                          train=False)\n",
    "        # Loaders\n",
    "        siamese_train_loader = torch.utils.data.DataLoader(siamese_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "        siamese_test_loader = torch.utils.data.DataLoader(siamese_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        # Instantiate model\n",
    "        n_samples, n_features = siamese_train_dataset.train_data.shape\n",
    "        for i in range(3):\n",
    "            nmodel = 'model_{}'.format(i)\n",
    "            print(\"\\t{}\".format(nmodel))\n",
    "            embedding_net = EmbeddingNet(n_features, 2)\n",
    "            model = SiameseNet(embedding_net)\n",
    "            if cuda:\n",
    "                model.cuda()\n",
    "            # Parameters\n",
    "            margin = 1.\n",
    "            loss_fn = ContrastiveLoss(margin)\n",
    "            lr = 1e-3\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "            n_epochs = 10\n",
    "            log_interval = round(len(siamese_train_dataset)/4/batch_size)\n",
    "            # Train\n",
    "            train_loss, val_loss = fit(siamese_train_loader, siamese_test_loader, model, loss_fn, optimizer, scheduler, \n",
    "                                       n_epochs, cuda, log_interval)\n",
    "            # Test Embeddings\n",
    "            val_embeddings_baseline, val_labels_baseline = vis.extract_embeddings(siamese_test_dataset.test_data, siamese_test_dataset.labels, model)\n",
    "            # Evaluation\n",
    "            n_clusters = len(np.unique(test_labels))\n",
    "            kmeans = KMeans(n_clusters=n_clusters)\n",
    "            siamese_clusters = kmeans.fit_predict(val_embeddings_baseline)\n",
    "            anmi = ANMI(siamese_clusters, val_labels_baseline)\n",
    "            # Store\n",
    "            meta_data['n_features'].append(len(feat))\n",
    "            meta_data['model'].append(nmodel)\n",
    "            meta_data['ANMI'].append(anmi)\n",
    "            exp_data[nmodel] = {'data': (val_embeddings_baseline, val_labels_baseline),\n",
    "                                'loss': (train_loss, val_loss),\n",
    "                                'ANMI': anmi}\n",
    "        pd.to_pickle(exp_data, os.path.join(exp_dir, \"model_{}.pkl\".format(len(feat))))\n",
    "    pd.to_pickle(meta_data, os.path.join(exp_dir, \"model_meta_data.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_training(train_data, train_labels, test_data, test_labels, feature_idx, exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pd.read_pickle(\"/srv/nas/mk2/projects/pan-cancer/experiments/meta_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
