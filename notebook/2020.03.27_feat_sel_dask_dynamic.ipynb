{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import datetime\n",
    "import pathlib\n",
    "\n",
    "from collections import OrderedDict \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Custom\n",
    "from dutils import Experiment\n",
    "from trainer import fit\n",
    "import visualization as vis\n",
    "from tcga_datasets import SiameseDataset\n",
    "\n",
    "# Models\n",
    "from tcga_networks import EmbeddingNet, SiameseNet\n",
    "from losses import ContrastiveLoss\n",
    "\n",
    "# Metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_mutual_info_score as ANMI\n",
    "\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTCGA(disease):\n",
    "    path = \"/srv/nas/mk2/projects/pan-cancer/TCGA_CCLE_GCP/TCGA/TCGA_{}_counts.tsv.gz\"\n",
    "    files = [path.format(d) for d in disease]\n",
    "    return files\n",
    "\n",
    "\n",
    "def readGCP(files, biotype='protein_coding', mean=True):\n",
    "    \"\"\"\n",
    "    Paths to count matrices.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    for f in files:\n",
    "        key = os.path.basename(f).split(\"_\")[1]\n",
    "        data = pd.read_csv(f, sep='\\t', index_col=0)\n",
    "        # transcript metadata\n",
    "        meta = pd.DataFrame([row[:-1] for row in data.index.str.split(\"|\")],\n",
    "                            columns=['ENST', 'ENSG', 'OTTHUMG', 'OTTHUMT', 'GENE-NUM', 'GENE', 'BP', 'BIOTYPE'])\n",
    "        meta = pd.MultiIndex.from_frame(meta)\n",
    "        data.index = meta\n",
    "        # subset transcripts\n",
    "        data = data.xs(key=biotype, level='BIOTYPE')\n",
    "        data = data.droplevel(['ENST', 'ENSG', 'OTTHUMG', 'OTTHUMT', 'GENE-NUM', 'BP'])\n",
    "        # average gene expression of splice variants\n",
    "        data = data.T\n",
    "        if mean:\n",
    "            data = data.groupby(by=data.columns, axis=1).mean()\n",
    "        data_dict[key] = data\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def uq_norm(df, q=0.75):\n",
    "    \"\"\"\n",
    "    Upper quartile normalization of GEX for samples.\n",
    "    \"\"\"\n",
    "    quantiles = df.quantile(q=q, axis=1)\n",
    "    norm = df.divide(quantiles, axis=0)\n",
    "    return norm\n",
    "\n",
    "\n",
    "def process_TCGA(disease=['BRCA', 'LUAD', 'KIRC', 'THCA', 'PRAD', 'SKCM']):\n",
    "    base=\"/srv/nas/mk2/projects/pan-cancer/TCGA_CCLE_GCP\"\n",
    "    # get files\n",
    "    tcga_files = getTCGA(disease)\n",
    "    # read meta/data\n",
    "    tcga_meta = pd.read_csv(os.path.join(base, \"TCGA/TCGA_GDC_ID_MAP.tsv\"), sep=\"\\t\")\n",
    "    tcga_raw = readGCP(tcga_files, mean=True)\n",
    "    # combine samples\n",
    "    tcga_raw = pd.concat(tcga_raw.values())\n",
    "    # Upper quartile normalization\n",
    "    tcga_raw = uq_norm(tcga_raw)\n",
    "    # log norm\n",
    "    tcga = tcga_raw.transform(np.log1p)\n",
    "    return tcga, tcga_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fsets(data, n_features, steps=5):\n",
    "    r = np.linspace(start=1, stop=n_features, num=steps, dtype='int')\n",
    "    idx = [np.random.choice(data.shape[1], size=i, replace=False) for i in r]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(n_features, embedding_dim, margin, lr, device):\n",
    "    embedding_net = EmbeddingNet(n_features, embedding_dim)\n",
    "    model = SiameseNet(embedding_net)\n",
    "    model.cuda(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_eval(test_embeddings, test_labels, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    siamese_clusters = kmeans.fit_predict(test_embeddings)\n",
    "    anmi = ANMI(siamese_clusters, test_labels)\n",
    "    return anmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_training(train_data, train_labels, test_data, test_labels, \n",
    "                     feature_idx, exp_dir, \n",
    "                     devices, embedding_dim, n_epochs, cuda=True):\n",
    "    # Meta data\n",
    "    meta_data = {\"n_features\":[],\n",
    "                 \"model\":[],\n",
    "                 \"ANMI\":[]}\n",
    "    # Params\n",
    "    batch_size = 8\n",
    "    kwargs = {'num_workers': 10, 'pin_memory': True} if cuda else {'num_workers': 10}\n",
    "    \n",
    "    # Feature Index\n",
    "    for batch, feat in enumerate(feature_idx):\n",
    "        print(\"Number features: {}\\n\".format(len(feat)))\n",
    "        exp_data = {'feature_idx':feat}\n",
    "        # Define data\n",
    "        siamese_train_dataset = SiameseDataset(data=train_data.iloc[:,feat],\n",
    "                                           labels=train_labels,\n",
    "                                           train=True)\n",
    "        siamese_test_dataset = SiameseDataset(data=test_data.iloc[:,feat],\n",
    "                                          labels=test_labels,\n",
    "                                          train=False)\n",
    "        # Loaders\n",
    "        siamese_train_loader = torch.utils.data.DataLoader(siamese_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "        siamese_test_loader = torch.utils.data.DataLoader(siamese_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        # Instantiate model\n",
    "        n_samples, n_features = siamese_train_dataset.train_data.shape\n",
    "        # Parameters\n",
    "        margin = 1.\n",
    "        lr = 1e-3\n",
    "        loss_fn = ContrastiveLoss(margin)\n",
    "        log_interval = round(len(siamese_train_dataset)/1/batch_size)\n",
    "        \n",
    "        if cuda:\n",
    "            models = [init_model(n_features,\n",
    "                                 embedding_dim,\n",
    "                                 margin=margin, \n",
    "                                 lr=lr, \n",
    "                                 device=d) for d in devices]\n",
    "        # Training\n",
    "        dlosses = [dask.delayed(fit)(siamese_train_loader, \n",
    "                                    siamese_test_loader, \n",
    "                                    model, \n",
    "                                    loss_fn, \n",
    "                                    optimizer, \n",
    "                                    scheduler, \n",
    "                                    n_epochs, \n",
    "                                    cuda, \n",
    "                                    log_interval) for model,optimizer,scheduler in models]\n",
    "        losses = dask.compute(*dlosses)\n",
    "        print('Done with losses')\n",
    "        # Test Embeddings\n",
    "        dembeddings = [dask.delayed(vis.extract_embeddings)(siamese_test_dataset.test_data, \n",
    "                                                            siamese_test_dataset.labels, \n",
    "                                                            model) for model,optimizer,scheduler in models]\n",
    "        embeddings = dask.compute(*dembeddings)\n",
    "        print('Done with embeddings')\n",
    "        # Evaluation\n",
    "        danmi_eval = [dask.delayed(cluster_eval)(test_embeddings, \n",
    "                                                test_labels, \n",
    "                                                len(np.unique(test_labels))) for test_embeddings,test_labels in embeddings]\n",
    "        anmi_eval = dask.compute(danmi_eval)\n",
    "        print('Done with eval')\n",
    "        # Store\n",
    "        for i,anmi in enumerate(anmi_eval):\n",
    "            nmodel = 'model_{}'.format(i)\n",
    "            meta_data['n_features'].append(len(feat))\n",
    "            meta_data['model'].append(nmodel)\n",
    "            meta_data['ANMI'].append(anmi)\n",
    "\n",
    "        exp_data[nmodel] = {'data': embeddings,\n",
    "                            'loss': losses,\n",
    "                            'ANMI': anmi_eval}\n",
    "        pd.to_pickle(exp_data, os.path.join(exp_dir, \"model_{}.pkl\".format(len(feat))))\n",
    "        \n",
    "    pd.to_pickle(meta_data, os.path.join(exp_dir, \"model_meta_data.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(disease, sample_type, **kwargs):\n",
    "    # GPUs\n",
    "    cuda = torch.cuda.is_available()\n",
    "    print(\"Cuda is available: {}\".format(cuda))\n",
    "    \n",
    "    # Read / write / process\n",
    "    tcga, tcga_meta = process_TCGA(disease)\n",
    "    # Feature design\n",
    "    feature_idx = generate_fsets(tcga, n_features=kwargs['n_features'], steps=kwargs['steps'])\n",
    "    # Experiment design\n",
    "    hierarchy = OrderedDict({'Disease':disease,\n",
    "                             'Sample Type':sample_type})\n",
    "    # Define experiment\n",
    "    exp = Experiment(meta_data=tcga_meta,\n",
    "                     hierarchy=hierarchy,\n",
    "                     index='CGHubAnalysisID',\n",
    "                     cases='Case ID',\n",
    "                     min_samples=20)\n",
    "    # Train / Test split\n",
    "    exp.train_test_split(cases='Case ID')\n",
    "    # Return data \n",
    "    train_data, train_labels = exp.get_data(tcga, subset=\"train\", dtype=np.float32)\n",
    "    test_data, test_labels = exp.get_data(tcga, subset=\"test\", dtype=np.float32)\n",
    "    \n",
    "    # Path *fix*\n",
    "    dtime = datetime.datetime.today().strftime(\"%Y.%m.%d_%H:%M\")\n",
    "    exp_dir = \"/srv/nas/mk2/projects/pan-cancer/experiments/test/{}_{}_{}_{}-{}\".format(dtime, \n",
    "                                                                                len(exp.labels_dict),\n",
    "                                                                                kwargs['embedding'],\n",
    "                                                                                kwargs['n_features'], \n",
    "                                                                                kwargs['steps'])\n",
    "    pathlib.Path(exp_dir).mkdir(parents=True, exist_ok=False)\n",
    "    print('Saving to: \\n{}'.format(exp_dir))\n",
    "    \n",
    "    # Meta data\n",
    "    experiments = {'experiment': exp,\n",
    "                   'train':(train_data, train_labels),\n",
    "                   'test': (test_data, test_labels)}\n",
    "    pd.to_pickle(experiments, os.path.join(exp_dir, \"experiment_meta_data.pkl\"))\n",
    "    \n",
    "    # Training\n",
    "    feature_training(train_data, train_labels, test_data, test_labels, \n",
    "                     feature_idx, exp_dir, \n",
    "                     kwargs['devices'], kwargs['embedding'], kwargs['n_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base=\"/srv/nas/mk2/projects/pan-cancer/TCGA_CCLE_GCP\"\n",
    "# read meta/data\n",
    "tcga_meta = pd.read_csv(os.path.join(base, \"TCGA/TCGA_GDC_ID_MAP.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BRCA', 'KIRC', 'LUAD', 'THCA', 'PRAD']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disease = tcga_meta[tcga_meta['Sample Type']=='Solid Tissue Normal']['Disease'].value_counts()\n",
    "# disease = list(disease[disease>=20].index)[:5]\n",
    "# disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = ['BRCA', 'LUAD', 'KIRC', 'THCA', 'PRAD', 'SKCM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_type = ['Primary Tumor', 'Solid Tissue Normal']\n",
    "params = {\"devices\":[3,4],\n",
    "          \"n_features\":2000,\n",
    "          \"steps\":100,\n",
    "          \"embedding\":2,\n",
    "          \"n_epochs\":10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: True\n",
      "Saving to: \n",
      "/srv/nas/mk2/projects/pan-cancer/experiments/test/2020.03.27_17:32_11_2_2000-100\n",
      "Number features: 1\n",
      "\n",
      "Train: [0/2931 (0%)]\tLoss: 0.311750\n",
      "Train: [0/2931 (0%)]\tLoss: 0.311601\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.170471\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.165399\n",
      "Epoch: 1/10. Train set: Average loss: 0.1709\n",
      "Epoch: 1/10. Validation set: Average loss: 0.1502\n",
      "Epoch: 1/10. Train set: Average loss: 0.1658\n",
      "Epoch: 1/10. Validation set: Average loss: 0.1896\n",
      "Train: [0/2931 (0%)]\tLoss: 0.129007\n",
      "Train: [0/2931 (0%)]\tLoss: 0.110255\n",
      "\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.162674\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.160165\n",
      "Epoch: 2/10. Train set: Average loss: 0.1625\n",
      "Epoch: 2/10. Validation set: Average loss: 0.1440Epoch: 2/10. Train set: Average loss: 0.1601\n",
      "Epoch: 2/10. Validation set: Average loss: 0.1966\n",
      "\n",
      "Train: [0/2931 (0%)]\tLoss: 0.246696\n",
      "Train: [0/2931 (0%)]\tLoss: 0.232569\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.150568\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.153689\n",
      "Epoch: 3/10. Train set: Average loss: 0.1539\n",
      "Epoch: 3/10. Validation set: Average loss: 0.1581\n",
      "Epoch: 3/10. Train set: Average loss: 0.1508\n",
      "Epoch: 3/10. Validation set: Average loss: 0.1432\n",
      "Train: [0/2931 (0%)]\tLoss: 0.283378\n",
      "Train: [0/2931 (0%)]\tLoss: 0.150790\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.146948\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.151361\n",
      "Epoch: 4/10. Train set: Average loss: 0.1473\n",
      "Epoch: 4/10. Validation set: Average loss: 0.1480\n",
      "Epoch: 4/10. Train set: Average loss: 0.1514\n",
      "Epoch: 4/10. Validation set: Average loss: 0.1448\n",
      "Train: [0/2931 (0%)]\tLoss: 0.213902\n",
      "Train: [0/2931 (0%)]\tLoss: 0.069525\n",
      "Epoch: 4/10. Train set: Average loss: 0.1514\n",
      "Epoch: 4/10. Validation set: Average loss: 0.1448\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.161768\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.149947\n",
      "Epoch: 5/10. Train set: Average loss: 0.1497\n",
      "Epoch: 5/10. Validation set: Average loss: 0.1652\n",
      "Epoch: 5/10. Train set: Average loss: 0.1619\n",
      "Epoch: 5/10. Validation set: Average loss: 0.2749\n",
      "Train: [0/2931 (0%)]\tLoss: 0.230925\n",
      "Train: [0/2931 (0%)]\tLoss: 0.162575\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.152783\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.148846\n",
      "Epoch: 6/10. Train set: Average loss: 0.1530\n",
      "Epoch: 6/10. Validation set: Average loss: 0.1452Epoch: 6/10. Train set: Average loss: 0.1489\n",
      "Epoch: 6/10. Validation set: Average loss: 0.1418\n",
      "\n",
      "Train: [0/2931 (0%)]\tLoss: 0.144013\n",
      "Train: [0/2931 (0%)]\tLoss: 0.158120\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.146427\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.161055\n",
      "Epoch: 7/10. Train set: Average loss: 0.1610\n",
      "Epoch: 7/10. Validation set: Average loss: 0.3284\n",
      "Epoch: 7/10. Train set: Average loss: 0.1464\n",
      "Epoch: 7/10. Validation set: Average loss: 0.1688\n",
      "Train: [0/2931 (0%)]\tLoss: 0.199739\n",
      "Train: [0/2931 (0%)]\tLoss: 0.115708\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.156024\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.147735\n",
      "Epoch: 8/10. Train set: Average loss: 0.1476\n",
      "Epoch: 8/10. Validation set: Average loss: 0.1698\n",
      "Train: [0/2931 (0%)]\tLoss: 0.083491\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.142200\n",
      "Epoch: 9/10. Train set: Average loss: 0.1420\n",
      "Epoch: 9/10. Validation set: Average loss: 0.1405\n",
      "Train: [0/2931 (0%)]\tLoss: 0.220598\n",
      "Train: [1098/2931 (100%)]\tLoss: 0.148326\n",
      "Epoch: 10/10. Train set: Average loss: 0.1485\n",
      "Epoch: 10/10. Validation set: Average loss: 0.1425\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8e7b64c4f691>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisease\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-816e2d6500a6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(disease, sample_type, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     feature_training(train_data, train_labels, test_data, test_labels, \n\u001b[1;32m     43\u001b[0m                      \u001b[0mfeature_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                      kwargs['devices'], kwargs['embedding'], kwargs['n_epochs'])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-484aae0c2311>\u001b[0m in \u001b[0;36mfeature_training\u001b[0;34m(train_data, train_labels, test_data, test_labels, feature_idx, exp_dir, devices, embedding_dim, n_epochs, cuda)\u001b[0m\n\u001b[1;32m     48\u001b[0m                                     \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                                     log_interval) for model,optimizer,scheduler in models]\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done with losses'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Test Embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_keys__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0mpostcomputes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/dask/threaded.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mget_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_thread_get_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mpack_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpack_exception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/dask/local.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0;31m# Main loop, wait on tasks to finish, insert new ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"waiting\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ready\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"running\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                     \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/dask/local.py\u001b[0m in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(disease=disease, sample_type=sample_type, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
